'''
TCN: Temporal Convulutional Network

The TCN is effectively a mix between an RNN and a CNN. The
TCN works by taking the output of a full passthrough of a
CNN and using it as the new input value for a new iteration
N times. It can be visualized like so:

     Input
       |
       V
  Input Layer <------+
       |             |
       |             |    
       |             |
       V             |
+--------------+     |
|     CNN      |     |
| Hidden Layer |     |
| Hidden Layer |     |
| Hidden Layer |     |
| Output Layer |     |
+--------------+     |
        |            |
        | Ouput data |
        +------------+

Generally, somewhere in or before the output layer, the
output data is padded. Padding isn't yet handled in my 
ConvulutionLayer1D implementation, so padding will be 
have to be done manually until something changes.

Convulutional functions are used here, because they 
allow the preservation of features that are present in
the data before the convolution, which reduces the size
of the data while preserving the features of the data,
freeing up space at the end so that (in the case of a fixed 
input length) we can append our next data in the sequence
to the end of the output, which creates our new input.

Try running the code below. Note this block:

data=np.array([1,2,3,4,5,6,7,8,9,10,11,12])
TCN_init(len(data))
print(f"Initial: {data}")
for i in range(20):
    data=TCN_loop(data)
    for i in range(10):
        data=np.append(data,[0])
    print(data)

Here's what the TCN functions do:
    - TCN_init prepares our network and defines 
      our kernel (or filter)
    - TCN_loop represents one passthrough of the
      network generated by TCN

The network is set to loop 20 times, and then the data 
is padded with zeroes between iterations. If you run the
code and look at the output, you'll notice that eventually,
convolving the layers begins to produce the same output of 
two values. This means we can continuously add new values 
to the input sequence for processing and have it collapse 
down to just a few to leave us space to add more data, all
while preserving the features of the original data.

Note: the code will give you different values each time.
This is due to the fact that, while the kernel is explicitly
defined, the bias values in the layer are randomized. If you
remove the kernel definitions from the code, the kernel will 
be randomized too. If you want to set biases, add this line
after each layer addition:

layer[<layer index>].bias = <val between -0.5 and 0.5>
'''

import numpy as np

from networkParts.network import Network
from networkParts.fcLayer import FCLayer
from networkParts.aLayer import ActivationLayer
from networkParts.aLayer import ActivationFunctions
from networkParts.lossFuncs import LossFunctions
from networkParts.convLayer import ConvolutionLayer1D

net=Network()

def calc_convolution_size(size_input, size_kernel, steps):
    val=size_input
    for i in range(steps):
        if val<size_kernel:
            raise Exception("The kernel size exceeded the input size. Ensure the kernel stays smaller than the input size.")
        val=(val-size_kernel)+1
    return val

layers=[]

def TCN_init(initial_input_size):
    layers.append(ConvolutionLayer1D(initial_input_size, 3, 1, "valid"))
    layers[0].weights=[0.5,-0.5,0.5]
    
    layers.append(ConvolutionLayer1D(calc_convolution_size(initial_input_size, 3, 1), 3, 1, "valid"))
    layers[1].weights=[0.5,-0.5,0.5]
    
    layers.append(ConvolutionLayer1D(calc_convolution_size(initial_input_size, 3, 2), 3, 1, "valid"))
    layers[2].weights=[0.5,-0.5,0.5]
    
    layers.append(ConvolutionLayer1D(calc_convolution_size(initial_input_size, 3, 3), 3, 1, "valid"))
    layers[3].weights=[0.5,-0.5,0.5]
    
    layers.append(ConvolutionLayer1D(calc_convolution_size(initial_input_size, 3, 4), 3, 1, "valid"))
    layers[4].weights=[0.5,-0.5,0.5]

    return True

def TCN_loop(input_data):
    out=input_data
    for l in layers:
        #print(layers.index(l))
        #print(f"[*] Weights: {l.weights}, Bias: {l.bias}, Len Bias: {len(l.bias)}")
        out=l.forward_propagation(out)[0]
        #print(f"[Layer {layers.index(l)}] {out=}")
    

    '''for i in range(len(input_data)-len(out)):
        out.append(0)
    print(out)'''
    return out

data=np.array([1,2,3,4,5,6,7,8,9,10,11,12])
TCN_init(len(data))
print(f"Initial: {data}")
for i in range(20):
    data=TCN_loop(data)
    for i in range(10):
        data=np.append(data,[0])
    print(data)